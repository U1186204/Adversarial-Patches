{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10eb86e0",
      "metadata": {
        "id": "10eb86e0"
      },
      "source": [
        "# AIPI 590 - XAI | Adversarial Patches\n",
        "\n",
        "Description: Here I create an adversarial patch from the 'Llama'class from Imagenet. The core idea is to create and test this patch to deceive machine learning models to assume test  images containing the patch are Llamas.\n",
        "\n",
        "Christian Moreira\n",
        "\n",
        "[![GitHub Repo](https://img.shields.io/badge/GitHub-Repo-black.svg?logo=github&style=for-the-badge)](https://github.com/U1186204/Adversarial-Patches/tree/main)\n",
        "\n",
        "[![Open In Colab](https://img.shields.io/badge/Open%20In-Colab-F9AB00.svg?logo=googlecolab&style=for-the-badge)](https://colab.research.google.com/github/U1186204/Adversarial-Patches/blob/main/main.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "078f4f3c",
      "metadata": {
        "id": "078f4f3c"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea61488b",
      "metadata": {
        "id": "ea61488b"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "%matplotlib inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg', 'pdf')\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError:\n",
        "    !pip install --quiet pytorch_lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "DATASET_PATH = \"data\"\n",
        "CHECKPOINT_PATH = \"saved_models/tutorial10\"\n",
        "CUSTOM_PATCH_PATH = os.path.join(CHECKPOINT_PATH, \"custom\")\n",
        "os.makedirs(CUSTOM_PATCH_PATH, exist_ok=True)\n",
        "\n",
        "pl.seed_everything(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)\n",
        "print(f\"Custom patch will be saved to: {CUSTOM_PATCH_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Datasets and Pretrained Models"
      ],
      "metadata": {
        "id": "mTZwk-EBaahT"
      },
      "id": "mTZwk-EBaahT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd9fdc4",
      "metadata": {
        "id": "4fd9fdc4"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import zipfile\n",
        "\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\"), (CHECKPOINT_PATH, \"patches.zip\")]\n",
        "\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "for dir_name, file_name in pretrained_files:\n",
        "    file_path = os.path.join(dir_name, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
        "        if file_name.endswith(\".zip\"):\n",
        "            print(\"Unzipping file...\")\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                # Corrected unzipping line\n",
        "                zip_ref.extractall(dir_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pretrained Model"
      ],
      "metadata": {
        "id": "J1VgSkCRaipu"
      },
      "id": "J1VgSkCRaipu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04615dd4",
      "metadata": {
        "id": "04615dd4"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Resnet35 it's a convolutional neural network for image classification that uses special \"shortcut\" connections to effectively train a network."
      ],
      "metadata": {
        "id": "CQYrmrqpaqRA"
      },
      "id": "CQYrmrqpaqRA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load ImageNet Data and Class Labels\n",
        "\n",
        "[![Gemini Log](https://img.shields.io/badge/Gemini-LLM_Log-blue.svg?logo=googleai&style=for-the-badge)](https://github.com/U1186204/Adversarial-Patches/blob/main/llm_log.txt)"
      ],
      "metadata": {
        "id": "BIUJ6uTGbDfF"
      },
      "id": "BIUJ6uTGbDfF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce9040b",
      "metadata": {
        "id": "5ce9040b"
      },
      "outputs": [],
      "source": [
        "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN,\n",
        "                         std=NORM_STD)])\n",
        "\n",
        "imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
        "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
        "                                     f\"Did you run Cell 2? Is 'TinyImageNet' in your 'data' folder?\"\n",
        "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "\n",
        "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
        "\n",
        "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
        "    label_names = json.load(f)\n",
        "\n",
        "def get_label_index(lab_str):\n",
        "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
        "    return label_names.index(lab_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Target Class"
      ],
      "metadata": {
        "id": "F5viMXERbNrd"
      },
      "id": "F5viMXERbNrd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de3f8b9",
      "metadata": {
        "id": "6de3f8b9"
      },
      "outputs": [],
      "source": [
        "target_class_name = 'llama'\n",
        "target_class_idx = 355\n",
        "\n",
        "assert target_class_name in label_names, \"Target class name not in label list!\"\n",
        "\n",
        "PATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** In this case I selected the target class Llama which has index in ImageNET of 355"
      ],
      "metadata": {
        "id": "C3_9r7OXbRpU"
      },
      "id": "C3_9r7OXbRpU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Baseline Evaluation and Visualization Functions\n",
        "\n",
        "[![Gemini Log](https://img.shields.io/badge/Gemini-LLM_Log-blue.svg?logo=googleai&style=for-the-badge)](https://github.com/U1186204/Adversarial-Patches/blob/main/llm_log.txt)"
      ],
      "metadata": {
        "id": "0fmu-FXfbk1b"
      },
      "id": "0fmu-FXfbk1b"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eval_model(dataset_loader, img_func=None):\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    for imgs, labels in tqdm(dataset_loader, desc=\"Validating...\"):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        if img_func is not None:\n",
        "            imgs = img_func(imgs, labels)\n",
        "        with torch.no_grad():\n",
        "            preds = pretrained_model(imgs)\n",
        "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
        "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        counter += preds.shape[0]\n",
        "    acc = tp.float().item()/counter\n",
        "    top5 = tp_5.float().item()/counter\n",
        "    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n",
        "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
        "    return acc, top5\n",
        "\n",
        "def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "        label = label.item()\n",
        "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
        "        pred = torch.softmax(pred, dim=-1)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title(label_names[label])\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
        "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
        "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
        "    ax[-1].set_yticks(np.arange(K))\n",
        "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
        "    ax[-1].invert_yaxis()\n",
        "    ax[-1].set_xlabel('Confidence')\n",
        "    ax[-1].set_title('Predictions')\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "exmp_batch, label_batch = next(iter(data_loader))"
      ],
      "metadata": {
        "id": "oYW40_UZNGOa"
      },
      "id": "oYW40_UZNGOa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About this Function**\n",
        "\n",
        "This function evaluates the model's performance on a given dataset by calculating its Top1 and Top5 accuracy. It can also apply an image modification function, like an adversarial patch, to the images before running the evaluation to see how well the attack worked."
      ],
      "metadata": {
        "id": "r_4oFwc9cZgq"
      },
      "id": "r_4oFwc9cZgq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Core Patch Attack Functions (Transform, Placement, Evaluation)\n",
        "\n",
        "[![Gemini Log](https://img.shields.io/badge/Gemini-LLM_Log-blue.svg?logo=googleai&style=for-the-badge)](https://github.com/U1186204/Adversarial-Patches/blob/main/llm_log.txt)"
      ],
      "metadata": {
        "id": "NXHN0ZREbyHh"
      },
      "id": "NXHN0ZREbyHh"
    },
    {
      "cell_type": "code",
      "source": [
        "TENSOR_MEANS = torch.FloatTensor(NORM_MEAN).to(device)[:,None,None]\n",
        "TENSOR_STD = torch.FloatTensor(NORM_STD).to(device)[:,None,None]\n",
        "\n",
        "def patch_forward(patch):\n",
        "    patch_0_1 = (torch.tanh(patch) + 1) / 2\n",
        "    return (patch_0_1 - TENSOR_MEANS) / TENSOR_STD\n",
        "\n",
        "def place_patch(img, patch):\n",
        "    patch_img = patch_forward(patch)\n",
        "\n",
        "    patch_height = patch.shape[1]\n",
        "    patch_width = patch.shape[2]\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        mask = torch.ones_like(patch_img)\n",
        "\n",
        "        angle = float(torch.empty(1).uniform_(-20, 20))\n",
        "        patch_rotated = TF.rotate(patch_img, angle, fill=0)\n",
        "        mask_rotated = TF.rotate(mask, angle, fill=0)\n",
        "\n",
        "        scale_factor = float(torch.empty(1).uniform_(0.75, 1.25))\n",
        "        new_height = int(patch_height * scale_factor)\n",
        "        new_width = int(patch_width * scale_factor)\n",
        "\n",
        "        if new_height < 1: new_height = 1\n",
        "        if new_width < 1: new_width = 1\n",
        "\n",
        "        patch_transformed = TF.resize(patch_rotated, [new_height, new_width], antialias=True)\n",
        "        mask_transformed = TF.resize(mask_rotated, [new_height, new_width], antialias=True)\n",
        "        mask_transformed = (mask_transformed > 0.5).float()\n",
        "\n",
        "        if new_height >= img.shape[2]: h_offset = 0\n",
        "        else: h_offset = np.random.randint(0, img.shape[2] - new_height - 1)\n",
        "\n",
        "        if new_width >= img.shape[3]: w_offset = 0\n",
        "        else: w_offset = np.random.randint(0, img.shape[3] - new_width - 1)\n",
        "\n",
        "        target_area = img[i, :, h_offset : h_offset + new_height, w_offset : w_offset + new_width]\n",
        "\n",
        "        img[i, :, h_offset : h_offset + new_height, w_offset : w_offset + new_width] = \\\n",
        "            (1.0 - mask_transformed) * target_area + mask_transformed * patch_transformed\n",
        "\n",
        "    return img\n",
        "\n",
        "def eval_patch(model, patch, val_loader, target_class):\n",
        "    model.eval()\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
        "            for _ in range(4):\n",
        "                img_labels = img_labels.to(device)\n",
        "                img_device = img.clone().to(device)\n",
        "                patch_img = place_patch(img_device, patch)\n",
        "\n",
        "                pred = model(patch_img)\n",
        "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
        "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
        "                counter += (img_labels != target_class).sum()\n",
        "\n",
        "    if counter.item() == 0:\n",
        "        return torch.tensor(0.0), torch.tensor(0.0)\n",
        "\n",
        "    acc = tp.float() / counter\n",
        "    top5 = tp_5.float() / counter\n",
        "    return acc, top5"
      ],
      "metadata": {
        "id": "hKBud5WXNGzP"
      },
      "id": "hKBud5WXNGzP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**place_patch function:** takes the patch, normalizes it, and applies random rotations and scaling before pasting it onto a location on an image.\n",
        "\n",
        "\n",
        "**eval_patch function:** then uses this random placement process to measure the patch's effectiveness, calculating the Top1 and Top5 accuracy for how often it fools the model into predicting the target class."
      ],
      "metadata": {
        "id": "-fyrUfIKczKN"
      },
      "id": "-fyrUfIKczKN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Main patch_attack Training Function\n",
        "\n",
        "[![Gemini Log](https://img.shields.io/badge/Gemini-LLM_Log-blue.svg?logo=googleai&style=for-the-badge)](https://github.com/U1186204/Adversarial-Patches/blob/main/llm_log.txt)"
      ],
      "metadata": {
        "id": "Q6yOWMWwb1iM"
      },
      "id": "Q6yOWMWwb1iM"
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=0)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
        "\n",
        "    if not isinstance(patch_size, tuple):\n",
        "        patch_size = (patch_size, patch_size)\n",
        "\n",
        "    patch = nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1], device=device), requires_grad=True)\n",
        "\n",
        "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
        "\n",
        "    # ---!!! REVERTING TO THE \"SMARTER\" SCHEDULER !!!---\n",
        "    # This will wait for the loss to stop improving before cutting the LR.\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"Starting attack. Tuning patch to target class '{label_names[target_class]}'\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss_sum = 0.0\n",
        "\n",
        "        for img, _ in train_loader:\n",
        "            img = img.to(device)\n",
        "            img = place_patch(img, patch)\n",
        "\n",
        "            pred = model(img)\n",
        "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
        "            loss = loss_module(pred, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_sum += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss_sum / len(train_loader)\n",
        "\n",
        "        # ---!!! SCHEDULER NOW NEEDS THE LOSS TO MAKE A DECISION !!!---\n",
        "        scheduler.step(avg_train_loss)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1} finished. Avg Loss: {avg_train_loss:4.2f}, Current LR: {current_lr}\")\n",
        "\n",
        "    print(\"Training finished. Running final validation...\")\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "\n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}"
      ],
      "metadata": {
        "id": "U3hhIaGcNY2I"
      },
      "id": "U3hhIaGcNY2I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is the main training loop that creates the adversarial patch from a blank slate over several epochs.\n",
        "\n",
        "In each epoch, it repeatedly places the patch on batches of images, calculates how far the model's prediction is from the target class, and then updates the patch with an optimizer to get better at fooling the model."
      ],
      "metadata": {
        "id": "WL7q_vhLdNCJ"
      },
      "id": "WL7q_vhLdNCJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the Adversarial Patch Attack"
      ],
      "metadata": {
        "id": "f8OQs107b9Nc"
      },
      "id": "f8OQs107b9Nc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EPOCHS_TO_TRAIN = 15\n",
        "\n",
        "patch_tensor, val_results = patch_attack(\n",
        "    pretrained_model,\n",
        "    target_class=target_class_idx,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    num_epochs=EPOCHS_TO_TRAIN\n",
        ")\n",
        "\n",
        "print(\"\\n---!!! TRAINING COMPLETE !!!---\")\n",
        "print(f\"Validation results for '{target_class_name}':\")\n",
        "print(f\"  Top-1 Fooling Accuracy: {val_results['acc']*100.0:4.2f}%\")\n",
        "print(f\"  Top-5 Fooling Accuracy: {val_results['top5']*100.0:4.2f}%\")\n",
        "\n",
        "# Save the patch tensor\n",
        "patch_filename = os.path.join(CUSTOM_PATCH_PATH, f\"{target_class_name}_patch.pt\")\n",
        "torch.save(patch_tensor, patch_filename)\n",
        "print(f\"Patch tensor saved to {patch_filename}\")"
      ],
      "metadata": {
        "id": "DtK_sdVDNkRT"
      },
      "id": "DtK_sdVDNkRT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation\n",
        "\n",
        "- From above we see the effectiveness of the code in reducing the average loss function on each of the epochs it ran starting as a loss of ~12% and dropping it to ~1.4%.\n",
        "\n",
        "- Once trainig completes, it is estimated that when the patch is placed on an image that 74.7% of the times 'Llama' was considered as the top class prediction.\n",
        "- In 92% of the time, 'Llama' was considered a top 5 prediciton"
      ],
      "metadata": {
        "id": "t6KXiC6KdYi1"
      },
      "id": "t6KXiC6KdYi1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What does the patch look like?"
      ],
      "metadata": {
        "id": "iEv-BW3LcEp4"
      },
      "id": "iEv-BW3LcEp4"
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_patch(patch_tensor):\n",
        "    patch_0_1 = (torch.tanh(patch_tensor.cpu()) + 1) / 2\n",
        "    patch_np = patch_0_1.permute(1, 2, 0).numpy()\n",
        "    patch_np = np.clip(patch_np, a_min=0.0, a_max=1.0)\n",
        "    return patch_np\n",
        "\n",
        "adversarial_patch_np = visualize_patch(patch_tensor)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(adversarial_patch_np)\n",
        "plt.title(f\"Functional '{target_class_name}' Patch\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "final_patch_img = Image.fromarray((adversarial_patch_np * 255).astype(np.uint8))\n",
        "png_filename = os.path.join(CUSTOM_PATCH_PATH, f\"FINAL_{target_class_name}_patch.png\")\n",
        "final_patch_img.save(png_filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "nP3cEQPzddkn"
      },
      "id": "nP3cEQPzddkn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Patch by placing it in Dataset Images\n",
        "\n",
        "[![Gemini Log](https://img.shields.io/badge/Gemini-LLM_Log-blue.svg?logo=googleai&style=for-the-badge)](https://github.com/U1186204/Adversarial-Patches/blob/main/llm_log.txt)"
      ],
      "metadata": {
        "id": "5ZjvrkCdcNak"
      },
      "id": "5ZjvrkCdcNak"
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_patch_attack_test(patch_tensor, title):\n",
        "    print(f\"\\n--- {title} ---\")\n",
        "\n",
        "    patch_batch = exmp_batch.clone().to(device)\n",
        "    patch_tensor_device = patch_tensor.to(device)\n",
        "    patch_batch = place_patch(patch_batch, patch_tensor_device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        patch_preds = pretrained_model(patch_batch)\n",
        "\n",
        "    for i in range(2, 32, 5):\n",
        "        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])\n",
        "\n",
        "perform_patch_attack_test(patch_tensor, f\"Adversarial Patch (Target: '{target_class_name}')\")"
      ],
      "metadata": {
        "id": "gf_Toy7add8K"
      },
      "id": "gf_Toy7add8K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation\n",
        "\n",
        "- The reuslts here demonstrate how our image classification model would determine the model class when a patch is placed in a random location of the image.\n",
        "\n",
        "- We see the 'Llama' is the top predicted class when it was placed on top of the 'electric ray, 'hammerhead', 'tigershark', 'tench', and 'goldfish' Images.\n",
        "\n",
        "- We patch does have its limitations though: when it was placed on a great white shark image it was not able to deceive the image prediction model.\n",
        "\n",
        "- Note there are various elements that affect patch performance and training such as rotation, image size, and learning rate. When optimizing my patch creation function oftentimes making image rotation too flexible or the learning rate too big/small would have major effects on the training ability to reduce loss. I found a good equilibrium of performance by setting my rotation angle range between (-20 to 20) degrees, keeping the learning rate at .1, and training the patch at by having it scale form .75X to 1.25X its size.\n"
      ],
      "metadata": {
        "id": "Q4B7siIjeHRz"
      },
      "id": "Q4B7siIjeHRz"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}